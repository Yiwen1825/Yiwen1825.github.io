[{"title":"2024picoCTF General Skills writeups","url":"/2025/04/08/2024picoCTF-General-Skills/","content":"\n> write by y1w3n(2024.05)\n\n## General Skills\n\n### Super SSH\n\n#### 題目說明\n\n![image](https://hackmd.io/_uploads/BJX3Oul06.png)\n\n#### solve\n\n照題目說的做就得到flag了\n![image](https://hackmd.io/_uploads/H1XMtugRp.png)\n\n#### answer\n\n``picoCTF{s3cur3_c0nn3ct10n_07a987ac}``\n\n#### note\n\n* 輸入密碼的時候它不會顯示出來你打什麼\n\n### Time Machine\n\n#### 題目說明\n\n![image](https://hackmd.io/_uploads/B1TX1ogCp.png)\n\n* challenge\n  * message.txt\n\n#### solve\n\n* ``git log`` : 查看提交歷史\n![image](https://hackmd.io/_uploads/SkPtyigCT.png)\n得到flag\n\n#### answer\n\n``picoCTF{t1m3m@ch1n3_8defe16a}``\n\n#### note\n\n### Commitment Issues\n\n#### 題目說明\n\n![image](https://hackmd.io/_uploads/H1ksG_lCT.png)\n\n* 一個壓縮的資料夾: challenge.zip\n\n#### solve\n\n他說他刪掉了flag所以我用``git log``查看提交歷史\n![image](https://hackmd.io/_uploads/BywIrue0a.png)\n看到``picoCTF <ops@picoctf.com>``他在3/12創建了flag後又刪掉了\n因此用``git show``，顯示指定提交的詳細訊息\n![image](https://hackmd.io/_uploads/B1XaI_e0a.png)\n\n#### answer\n\n``picoCTF{s@n1t1z3_cf09a485}``\n\n#### note\n\n* ``git``: 版本控制\n[常用指令](https://hellojs-tw.github.io/git-101/cheat-sheet.html)\n\n### Verify\n\n#### 題目說明\n\n![image](https://hackmd.io/_uploads/SJlA4ceCa.png)\n\n* challenge\n  * files(資料夾，裡面放了一堆要比對的文件)\n  * decrypt. sh\n  * checksum.txt\n    ![image](https://hackmd.io/_uploads/HyIlI9gC6.png)\n\n#### solve\n\n要找到跟checksum.txt裡面hash值一樣的檔案\n\n* 使用``sha256sum files/* | grep 3ad37ed6c5ab81d31e4c94ae611e0adf2e9e3e6bee55804ebc7f386283e366a4\n3ad37ed6c5ab81d31e4c94ae611e0adf2e9e3e6bee55804ebc7f386283e366a4  files/e018b574\n``\n![image](https://hackmd.io/_uploads/SyR2w5lAp.png)\n看到files裡e018b574的文件符合條件\n\n接著用它給的decrypt. sh解密\n![image](https://hackmd.io/_uploads/H1MlOcg0T.png)\n\n#### answer\n\n``picoCTF{trust_but_verify_e018b574}``\n\n#### note\n\n* ``sha256sum`` : 驗證文件\n    1. 計算文件的校驗和\n    2. 跟預期值做比較\n* ``file/*`` : file資料夾中所有檔案\n\n### Blame Game\n\n#### 題目說明\n\n![image](https://hackmd.io/_uploads/ry2ylqgAp.png)\n給一個壓縮過的challenge.zip裡面有一個message. py\n\n#### solve\n\n* ``git blame <file>`` : 顯示檔案的每一行是在哪個提交中被修改的，並顯示修改者的資訊\n![image](https://hackmd.io/_uploads/BkUdW9lAp.png)\n得到flag\n\n#### answer\n\n``picoCTF{@sk_th3_1nt3rn_cfca95b2}``\n\n### Collaborative Development\n\n#### 題目說明\n\n![image](https://hackmd.io/_uploads/BkGrJsVR6.png)\n給了一個flag. py ，裡面只有一行:``print(\"Printing the flag...\")``就沒了\n![image](https://hackmd.io/_uploads/rkZe_mHCT.png)\n\n#### solve\n\n* ``git branch -a``:看可用的分支\n    ![image](https://hackmd.io/_uploads/BylEd7rAT.png)\n    看到我現在在``main``，然後還有``feature/part-1``、``feature/part-2``、``feature/part-3``這幾個分支\n* ``git merge feature/part-1``\n    ![image](https://hackmd.io/_uploads/rkq0ZNrCp.png)\n    flag.py中有未提交的更改，而合併操作會覆蓋這些更改。\n  * 如何解決?\n    * ``git checkout -- flag.py`` : 撤銷修改\n    * 然後再執行一次``git merge feature/part-1``就好了，現在flag.py長這樣:\n        ![image](https://hackmd.io/_uploads/SJ4qmVrCa.png)\n* ``git merge feature/part-2``\n    ![image](https://hackmd.io/_uploads/BkZVU4SR6.png)\n    無法確定身份資訊（作者和提交者）\n  * 如何解決?\n    * 登入就好www(直接拿人家的XD)\n        ![image](https://hackmd.io/_uploads/By6aU4rRa.png)\n    * 然後再執行一次``git merge feature/part-2``就好了，現在flag.py長這樣:\n        ![image](https://hackmd.io/_uploads/ByDQPVBRa.png)\n* ``git merge feature/part-3``\n    ![image](https://hackmd.io/_uploads/r1cnv4SR6.png)\n    嘗試合併feature/part-3分支時發生了衝突，導致合併無法進行\n  * 如何解決?\n    * 打開flag. py，看到衝突標記，``<<<<<<< HEAD``、``=======``和``>>>>>>> feature/part-2``表示目前分支（main分支）和要合併的分支（feature/part-2分支）之間有衝突\n        ![image](https://hackmd.io/_uploads/Skrzi4S0T.png)\n    * 手動把衝突標記刪掉\n        ![image](https://hackmd.io/_uploads/r1KKiESRa.png)\n    * ``git add flag.py`` : 提交\n    * ``git merge --continue`` : 繼續\n    * 再執行一次``git merge feature/part-3``就好了，現在flag.py長這樣:\n        ![image](https://hackmd.io/_uploads/Bkiz3NBRT.png)\n* 已經有flag了但如果想做到完美就把這邊的合併衝突也手動解決掉\n    ![image](https://hackmd.io/_uploads/Bylxc3EHRT.png)\n* 執行\n    ![image](https://hackmd.io/_uploads/rJlCnESC6.png)\n\n#### answer\n\n``picoCTF{t3@mw0rk_m@k3s_th3_dr3@m_w0rk_7ae8dd33}``\n\n#### note\n\n分支是獨立的。使得團隊成員能夠在不互相干擾的情況下並行開發和合作。\n\n### binhexa\n\n#### 題目說明\n\n![image](https://hackmd.io/_uploads/rybcGxMCa.png)\n進去之後長這樣\n![image](https://hackmd.io/_uploads/rJtIBgfRT.png)\n就是它給了兩個二進制數字接著出題要我們答，共六題，我無聊玩了三次，每次出題那兩個數字是隨機的\n\n#### solve\n\n* Question 1/6:\n  * \">> 1\" 右循環1 bits: 把每個右移1 bits，最後一個補到最前面\n![image](https://hackmd.io/_uploads/HyiOBeMCT.png)\n* Question 2/6:\n  * '+'加法運算\n![image](https://hackmd.io/_uploads/B1cZUxMC6.png)\n* Question 3/6:\n  * '*' 乘法運算\n![image](https://hackmd.io/_uploads/S14BLlMCp.png)\n* Question 4/6:\n  * \"<< 1\" 左循環1 bits: 把每個左移1 bits，第一個捕到最後面\n![image](https://hackmd.io/_uploads/HyfOIez0p.png)\n* Question 5/6:\n  * AND運算: 2個都是1才會輸出1\n![image](https://hackmd.io/_uploads/SkJUvlfR6.png)\n* Question 6/6:\n  * OR運算: 其中一個是1結果就是1\n![image](https://hackmd.io/_uploads/BJSpPgfA6.png)\n\n* 最後要我給最後一題答案的16進制\n  * 11111111 -> ff\n![image](https://hackmd.io/_uploads/By3PuxGAa.png)\n最後得到flag\n\n#### answer\n\n``picoCTF{b1tw^3se_0p3eR@tI0n_su33essFuL_d9a7ddd2}``\n\n#### note\n\n[運算式運算子概念](https://ithelp.ithome.com.tw/articles/10214404)\n\n[二進制計算機](https://miniwebtool.com/zh-tw/binary-calculator/?number1=01110111&operate=3&number2=10011110)\n\n[進制轉換網站](https://www.kwuntung.net/hkunit/base/base.php)\n","tags":["picoCTF","General Skills"],"categories":["writeup"]},{"title":"2024picoCTF Reverse writeups","url":"/2025/04/08/2024picoCTF_Reverse/","content":"> write by y1w3n(2024.05)\n\n## Reverse Engineering\n\n### WinAntiDbg0x100\n\n#### 題目說明\n\n![image](https://hackmd.io/_uploads/rJdz9CkTC.png)\n> hint: Hints will be displayed to the Debug console. Good luck!\n\n![image](https://hackmd.io/_uploads/rJEDoDA3A.png)\n\n* 執行下來發現他叫我using a debugger\n\n#### slove\n\n* 進到IDA中看看哪裡會輸出flag\n* 發現這裡是判斷會不會輸出flag的地方\n![image](https://hackmd.io/_uploads/Byo4WIyaA.png)\n* 在```jz short loc_C6161B```設中斷點\n* 使用Debugger功能執行\n* 會發現直接執行後輸出:\n![image](https://hackmd.io/_uploads/HJBoArJaC.png)\n* 也就是說沒嘗試bypass不會跳到輸出flag的地方\n* 回去看看上面跳轉的條件\n![image](https://hackmd.io/_uploads/HJEByLJpA.png)\n  * ```call ds:IsDebuggerPresent```: 偵測有沒有debugger存在，沒有的話就return 0\n  * ```test eax, eax```: 概念和AND eax,eax差不多，不一樣的是test不會改變eax。<mark>現在我們有使用debugger所以值不為0</mark>\n        ![image](https://hackmd.io/_uploads/SJMWQ816C.png)\n  * ```jz```: 如果為0則跳轉到輸出flag的地方\n* 把eax值改成0\n![image](https://hackmd.io/_uploads/BkPPX8yaA.png)\n* 成功跳轉，得到答案\n![image](https://hackmd.io/_uploads/BkvbNLJa0.png)\n\n#### Answer\n\n```picoCTF{d3bug_f0r_th3_Win_0x100_cc0ff664}```\n\n### WinAntiDbg0x200\n\n#### 題目說明\n\n![image](https://hackmd.io/_uploads/BkByIIJpR.png)\n> hints: Hints will be displayed to the Debug console. Good luck!\n\n* 使用管理員權限執行\n![image](https://hackmd.io/_uploads/HkecsU1pR.png)\n* 跟前一題一樣要用debugger\n\n#### slove\n\n* 一開始有先判斷是否為管理員權限執行\n![image](https://hackmd.io/_uploads/By04Vak60.png)\n* 先看看哪裡的執行步驟會前往不輸出flag的路線\n![image](https://hackmd.io/_uploads/rJ6oVp1pA.png)\n  * ```test edx, edx```：測試 edx 是否為零\n  * ```jnz short loc_F11832```：如果結果為<mark>非零</mark>，則跳轉到```loc_F11832```\n  * ```loc_F11832```: 輸出### Oops! The debugger was detected. Try to bypass\n  * **現在edx為1，1!=0，會前往```loc_F11832```輸出錯誤**\n    ![image](https://hackmd.io/_uploads/HykCvaya0.png)\n* 修改其值，成功前往下一步\n![image](https://hackmd.io/_uploads/r1hSuaJTA.png)\n* 繼續往下看發現還有檢查是否在Debugger執行\n![image](https://hackmd.io/_uploads/H1hTupk60.png)\n  * ```call ds:IsDebuggerPresent```: 回傳值0（如果沒有Debugger）或 1（如果有Debugger）\n  * 現在eax值為1\n  * ```jz short loc_F11847```: 如果為0則跳轉到輸出flag的地方\n* 跟前一題一樣，修改eax暫存器值\n![image](https://hackmd.io/_uploads/ByZ-ZAka0.png)\n* 得到答案\n![image](https://hackmd.io/_uploads/SyQBbCkpA.png)\n\n#### Answer\n\n```picoCTF{0x200_debug_f0r_Win_c6db2768}```\n","tags":["picoCTF","Reverse"],"categories":["writeup"]},{"title":"Machine Learning","url":"/2025/04/07/Machine-Learning/","content":"# 機器學習\n\n> **BY y1w3n**\n---\n> **隨手記版本**\n\n定義問題->蒐集資料->資料分析->處理資料集->訓練模型->推論和預測\n\n## 名詞\n\n* <mark>監督式學習</mark >\n  * 透過特徵跟目標變數(正確答案)來學習\n  * KNN\n    * 分類器\n      * 資料放進去之後找離它最近的K筆資料，結果就會推測在被選中的K筆資料中找多的(分類)或平均(回歸)\n  * 決策樹\n    * 一棵可用於決策分類的樹形分類器\n  * 回歸 ： 預測有意義的數值\n    * for監督式學習\n    * 有意義的數值(例：鞋子尺寸23.5cm、24cm…)\n    * 沒意義的數值(例：性別男視為0、女視為1)\n  * 分類\n    * ![image](https://hackmd.io/_uploads/r1G_QeDXke.png)\n\n* <mark>非監督式學習</mark >\n  * **不**提供目標變數(正確答案)\n  * 所以就要去分析輸入的資料再去解釋分析出的結果\n  * 方法：降維(例如PCA)、分群(例：k-means )\n    * 降維：以最少特徵去分析資料的方法\n  * k-means(平均分群演算法)\n    1. 設定分群數目\n    2. 隨機挑選質心\n    3. 算距離\n    4. 分群資料(看哪些資料被分到哪個質心)\n    5. 重找質心(換到每群中間)\n    6. 重複3~5 一直到質心不再移動\n\n* 強化學習\n\n## 深度學習入門\n\n[一些colab連結](https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2&hl=zh-tw#scrollTo=WBPaKCarDk0r)\n\n### 感知器神經網路演算法\n\n* 依照人類大腦特性提出\n* 感知器\n  * 接收刺激(input)\n  * 神經元計算權重\n  * if超過閾值θ(界限)-> 輸出1 (0訊號不流/1流)\n  * if 傳遞訊號 = 神經元被激活\n        p.s.神經元只傳遞訊號\n  * 主要運用在二元分類\n  * 將訊息傳遞給其他神經元作為輸入\n\n    ```python\n    # 反及閘(NAND)感知器\n    import numpy as np\n    def AND(x1, x2):\n        x = np.array([x1, x2]) # 輸入\n        w = np.array([0.5, 0.5])#權重\n        theta = -0.7\n        tmp = np.sum(w*x) + theta\n        if tmp <= 0:\n           return 0\n        else:\n           return 1\n    ```\n\n  * 互斥或閘(XOR)在沒有隱藏層運作時不能表示\n    * 透過組合輸入NAND、OR後再傳遞到AND輸出可達成\n    * 就叫**多層感知器**\n    * 也就是說多層感知器<mark >能表示非線性空間</mark >(非線性空間包含一次以上多項式函數，圖形不是直線)\n        <details>\n          <summary> 舉例</summary>\n        x1、x2 : 第0層\n        反及閘(NAND)、或閘(OR)輸出 : 第1層\n        及閘(AND)輸出 : 第2層\n\n        | x1 | x2 | NAND| OR | AND |XOR(對照)|\n        |----|----|-----|----|-----|--------|\n        |  0 |  0 |   1 |  0 |   0 |    0   |\n        |  0 |  1 |   1 |  1 |   0 |    1   |\n        |  1 |  0 |   1 |  1 |   0 |    1   |\n        |  1 |  1 |   0 |  1 |   1 |    0   |\n\n        </details>\n\n### 神經網路\n\n* 輸入層->隱藏層(中間層)->輸出層\n* **激活函數h(x)** : 將輸入信號轉成輸出信號，必須是非線性函數，這樣疊加層才能發揮優勢(輸出層的激活函數用σ()表示)\n* 神經網路中是**sigmoid函數**作為激活函數\n* **階躍函數** : 激活函數以閾值為界，一旦輸入超過閾值，就切換輸出\n\n<details>\n  <summary>過程</summary>\n\n![image](https://hackmd.io/_uploads/ByFLmz__0.png)\n\n* a、y : 節點\n</details>\n\n<details>\n<summary>階躍函數圖形、sigmoid函數圖形</summary>\n\n```python\n#階躍函數圖形程式碼\nimport numpy as np\nimport matplotlib.pylab as plt\ndef step_function(x):\n  return np.array(x > 0)\n\nx = np.arange(-5.0, 5.0, 0.1)\ny = step_function(x)\nplt.plot(x, y)\nplt.ylim(-0.1, 1.1) # 指定y軸的範圍\nplt.show()\n```\n\n![image](https://hackmd.io/_uploads/S1imzyFuC.png)\n\n---\n\n```python\n# 階躍函數圖形+　sigmoid函數圖形程式碼\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\ndef step_function(x):\n    return np.array(x > 0, dtype=int)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nX = np.arange(-5.0, 5.0, 0.1)\ny_step_function = step_function(X)\ny_sigmoid = sigmoid(X)\n\nplt.figure(figsize=(8, 6))\nsns.lineplot(x=X, y=y_step_function, label='step_function')\nsns.lineplot(x=X, y=y_sigmoid, label='sigmoid')\n\nplt.ylim(-0.1, 1.1)\nplt.legend()\nplt.show()\n```\n\n![image](https://hackmd.io/_uploads/SksYdktu0.png)\n</details>\n\n* 階躍函數跟sigmoid函數比較\n  * 階躍函數只能return 0或1\n  * 感知器中神經源之間流動是二元信號(0或1)\n  * 神經網路是流動連續數值的信號\n  * 輸出信號都在0~1之間\n  * 都是非線性函數\n* ReLU（Rectified Linear Unit）函數\n  * 神經網路最近主要使用的函數\n  * 輸出大於0就輸出該值\n  * 小於等於0就輸出0\n\n<details>\n<summary>ReLU函數圖形</summary>\n\n```python\n#ReLU函數圖形\ndef relu(x):\n  return np.maximum(0, x) #maximum: 從輸入中選較大的值輸出\n\nx = np.arange(-5.0, 5.0, 0.1)\nprint(relu(x))\ny=relu(x)\nplt.plot(x, y)\nplt.ylim(-1, 5) # 指定y軸的範圍\nplt.show()\n```\n\n```python\n#print(relu(x))\n[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.2 0.3\n 0.4 0.5 0.6 0.7 0.8 0.9 1.  1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.  2.1\n 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.  3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9\n 4.  4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9]\n```\n\n![image](https://hackmd.io/_uploads/BJEyylK_0.png)\n</details>\n\n* 運算\n  * 權重的符號\n    ![image](https://hackmd.io/_uploads/H1m2XZtdC.png)\n    ![image](https://hackmd.io/_uploads/B1y3I-Y_A.png)\n[colab](https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=5NWMewF4gJ1P&line=4&uniqifier=1)\n\n```python\nX = np.array([1.0, 0.5])\nW1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\nB1 = np.array([0.1, 0.2, 0.3])\nprint(W1.shape) # (2, 3)\nprint(X.shape)  # (2,)\nprint(B1.shape) # (3,)\nA1 = np.dot(X, W1) + B1 # np.dot : 一次計算出結果\nprint(A1)\n#經過sigmoid激活函數\nZ1=sigmoid(A1)\nprint(Z1)\n\n```\n\n[三層神經網路程式](https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=WG-XG0WOjDOs&line=1&uniqifier=1)\n\n* 輸出層\n  * 回歸問題用恆等函數(依原樣輸出)\n  * 分類問題用softmax函數(但容易有超級大ㄉ值所以用下面的方法修)\n    <details>\n      <summary>這樣修的</summary>\n\n    ![image](https://hackmd.io/_uploads/ByayAWtu0.png)\n    分子和分母上都乘上C這個任意的常數（因為同時對分母和    分子乘以相同的常數，所以計算結果不變）。然後，把這個C    移到指數函數（exp）中，記為logC。最後，把logC替換為    另一個符號C'\n    C通常是取輸入訊號的最大值\n    </details>\n\n    ```python\n    #softmax函數\n      def softmax(a):\n          c = np.max(a)  \n          # exp(x)是表示ex的指數函數（e是納皮爾常數2.7182...）\n          exp_a = np.exp(a-c)\n          sum_exp_a = np.sum(exp_a)\n          y = exp_a / sum_exp_a\n          return y\n    ```\n\n  * softmax函數輸出介於0~1且輸出總和為1\n  * softmax函數也可以解釋為<mark>機率</mark>\n    <details>\n      <summary>以手寫數字識別為例</summary>\n\n    * 分10類->輸出層神經元數量10個\n    * 分類->softmax函數->用機率推理結果\n    ![image](https://hackmd.io/_uploads/BJ3rxGtOA.png)\n    * 2的輸出值會最大\n    </details>\n* 數據處理\n  * 正規化 : 把資料限定到某個範圍內\n  * 預處理 : 對神經網路的輸入數據進行某種既定的轉換\n  * 數據白化 : 將資料整體的分佈形狀均勻化的方法\n\n### 神經網路的學習\n\n* 從**數據**中學習\n* 提取特徵量(人類想的)\n  * 圖像的特徵量通常表示為向量的形式\n* 但深度學習在訓練過程不會有人為介入\n* 數據分為**訓練數據**和**測試數據**\n* 泛化能力: 處理未被觀察過的數據的能力\n* 過擬合: 無法處理新數據\n* one-hot label: 將類別數據轉換為二進制向量表示\n\n#### 損失函數\n\n [參考連結](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8-loss-function-2dcac5ebb6cb)\n\n* 損失就是「實際值和預測值的殘差」\n* 可以用任意函數\n* 越接近0越好\n* 一般用MSE(均方誤差)跟交叉熵誤差\n* <mark>表示性能不好的程度</mark>\n* 均方誤差(Mean-Square Error)\n  * 最常被用在回歸上的損失函數\n    ![image](https://hackmd.io/_uploads/S1NwRGYdC.png)\n  * 求預測值與真實值之間距離的平方總和\n\n    ```python\n    def mean_squared_error(y, t):\n        return 0.5 * np.sum((y-t)**2)\n    ```\n\n    [均方誤差舉例colab](https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=TGBChgU75EmJ&line=11&uniqifier=1)\n* 交叉熵誤差\n    ![image](https://hackmd.io/_uploads/rkurZQY_C.png)\n  * 分類問題常用的損失函數\n  * 正解標籤越大，交叉熵誤差算出來越接近0\n\n    ```python\n    def cross_entropy_error(y, t): \n      delta = 1e-7 #防止負無限大發生\n      return -np.sum(t * np.log(y + delta))\n    ```\n\n    [交叉熵誤差舉例colab](https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=F3G1dUL879Dk&line=12&uniqifier=1)\n* mini-batch學習 : 從一堆數據中選一批據進行學習\n\n#### 梯度\n\n[偏導數跟梯度動畫圖](https://www.youtube.com/watch?v=Dhou27Ergkk)\n\n* 目的: 尋找梯度為0的地方，找到的就是最小值或極小值(鞍點)\n* 梯度: 損失函數衡量模型預測與真實值之間的誤差，而梯度是用來最小化損失函數的工具。也就是說要<mark>找最優參數組合</mark>(權重跟偏置)\n* 導數: 某個函數在某一特定點的瞬間變化率\n  * 導數計算方式:對於一般的多項式 \\( ax^n \\)，其導數是 \\( anx^(n-1) \\)。\n<details>\n<summary>簡單解釋一下梯度</summary>\n梯度可以用比較簡單的方式理解成「多變數函數的斜率」。假設你爬山的時候有一張地圖，上面標出了高度變化的等高線。梯度就像是告訴你在某個位置，哪個方向是上坡最陡、上升最快的方向。\n\n想像一下你在一個山坡上，你想知道哪個方向爬得最快，梯度就告訴你哪個方向坡最陡，並且告訴你這個方向的陡峭程度。\n\n在數學上，假設有一個二維的高度函數 f(x,y)，梯度是由函數在x和 y 方向上的斜率（即偏導數）組成的向量∇𝑓(𝑥,𝑦)，它由兩部分組成：x 方向上的變化率（偏導數）和在y方向上的變化率（偏導數）。公式是：\n![image](https://hackmd.io/_uploads/ByKbJnJK0.png)\n**∂f/∂x: 在y固定的情況下f的變化量除以x的變化量就叫做f對於x的偏導數** (這個符號∂念:partial)\n![image](https://hackmd.io/_uploads/Syzwl59t0.png)\n\n使用偏導數在計算梯度的時候就像是在算每個點的斜率，我們要的就是用梯度找這個範圍的最小值\n\n導數跟偏導數差別:\n導數單變量函數、單一方向變化\n偏導數看多個方向上的變化\n</details>\n\n[梯度下降法colab](https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=4r8QQ_eZkuCE&line=24&uniqifier=1)\n\n* 算出梯度後，在梯度下降法中，帶入公式:\n![image](https://hackmd.io/_uploads/SymBBpJtR.png)\n  * θ是參數向量。\n  * α 是學習率。\n  * \\( ∇θ J(θ))是損失函數(J(θ))對參數θ的梯度。\n* 學習率\n  * 一開始我們要人工設學習率，通常是0.01或0.01\n  * 概念: 就好比要找的一個「好位置」，如果學習率太大的話容易跳過那個「好位置」，太小的話雖然準確但會非常慢甚至陷在局部最小值\n  * 什麼情況要更新學習率?\n    * 調小\n      * 損失在某段時間內沒有減少時\n      * 損失上下波動很大也就是不穩定時\n    * 調大\n      * 當損失函數下降得太慢時\n      * 陷入局部最小值時\n* 帶入上面的公式我會得到新的參數，然後再去計算損失函數跟梯度，就這樣重複\n\n#### 神經網路學習完整步驟\n\n* 目的: 找到合適的參數，也就是合適的偏置跟權重\n\n---\n\n1. mini-batch\n    * 從一堆數據中隨機選一批據進行學習，目標是縮小mini-batch損失函數的值\n2. 計算梯度\n    * 為了減少mini-batch的損失函數的值，需要求出各個權重參數的梯度。\n    * 梯度顯示了在當前 mini-batch 中，損失函數減少最快的方向。\n3. 更新參數\n    * 算出梯度後帶入公式更新參數(參數更新是對我當前mini-batch的數據)\n4. 重複以上步驟\n\n---\n\n* 這裡是用mini-batch做梯度下降所以也叫隨機梯度下降法(Stochastic Gradient Descent, SGD)\n* 簡單但費時\n![image](https://hackmd.io/_uploads/r1R4TmzFC.png)\n* 每經過一個epoch(當所有數據都被使用過時)，就記錄當下數據跟精確度(損失函數)\n\n---\n\n### 誤差反向傳播法\n\n* 被算在上面的第2步: 記算梯度\n\n#### 計算圖\n\n* 表示神經網路計算過程的結構化圖形\n![image](https://hackmd.io/_uploads/rycIKSbYA.png)\n* 正向傳播: 左到右計算(輸入-> 輸出)\n* 反向傳播: 從右到左，基於鍊式法則後向前計算導數\n* 局部計算: 只需要計算跟自己有關的內容不用考慮全局\n* 計算圖的反向傳播：沿著與正方向相反的方向，乘上局部導數\n\n#### 鍊式法則\n\n* 複合函數: 多個函數組成\n![image](https://hackmd.io/_uploads/HkR9hr-FA.png)\n* 複合函數導數=構成這個複合函數的函數們的**導數乘積**(在上面的圖中就是左邊兩個函數的導數相乘=右邊函數的導數)\n\n### 相關技巧\n\n#### 參數更新\n\n* 隨機梯度下降法(SGD)在某些時候效率低，我們可以優化它的下降路徑\n![image](https://hackmd.io/_uploads/r1GmLtjY0.png)\n* Momentum\n  * 動量優化法\n  * 在0~1之間(通常設0.9)\n* AdaGrad\n* Adam\n\n<details>\n\n<summary>圖</summary>\n\n![image](https://hackmd.io/_uploads/ryH76KsKC.png)\n![image](https://hackmd.io/_uploads/HkNBpFoKA.png)\n![image](https://hackmd.io/_uploads/BJv8pFsFR.png)\n![image](https://hackmd.io/_uploads/ryfv6KoY0.png)\n</details>\n","tags":["AI"],"categories":["note"]},{"title":"Hello World","url":"/2025/04/05/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n"},{"title":"Friends","url":"/Friends/index.html","content":"\n<div class=\"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-2 gap-6\">\n\n  <!-- Friend Card 1 -->\n  <div class=\"flex items-center border border-gray-600 rounded-xl p-4 bg-black hover:bg-gray-900 transition mb-6\">\n    <img src=\"https://cdn.discordapp.com/avatars/1007624670373220454/ab58519dc7368d0b2ef0f61410f09fd8.webp?size=256\" \n         alt=\"Sh1ro 頭貼\" \n         class=\"w-16 h-16 rounded-full\">\n    <div class=\"ml-4\">\n      <h2 class=\"text-lg font-bold\">\n        <a href=\"https://xiaobai0426.github.io/\" target=\"_blank\" class=\"text-hacker-color1 hover:underline\">\n          Sh1ro\n        </a>\n      </h2>\n      <p class=\"text-sm text-gray-400\">藍隊佬</p>\n    </div>\n  </div>\n\n  <!-- Friend Card 2 -->\n  <div class=\"flex items-center border border-gray-600 rounded-xl p-4 bg-black hover:bg-gray-900 transition\">\n    <img src=\"https://cdn.discordapp.com/avatars/585095932090122252/b0cbfa26bcef7789b5480e2dff11561e.webp?size=256\" \n         alt=\"kkoebi 頭貼\" \n         class=\"w-16 h-16 rounded-full\">\n    <div class=\"ml-4\">\n      <h2 class=\"text-lg font-bold\">\n        <a href=\"https://kkoebi.github.io/blog/\" target=\"_blank\" class=\"text-hacker-color1 hover:underline\">\n          kkoebi\n        </a>\n      </h2>\n      <p class=\"text-sm text-gray-400\">ML、WEB 佬</p>\n    </div>\n  </div>\n\n</div>"},{"title":"about","url":"/about/index.html","content":"## Introduction\n\nHi, I’m y1w3n! I usually go by y1w3n or yiwen1825 online. I’m currently a Computer Science student at NTHU, and I still have a lot to improve on. Anyway, nice to meet you!\n\n### Something I'm learning\n\n- Program languages\n  - python\n  - C++\n- Information Security\n  - Reverse - Game hacking\n  - Reverse - Malware Analysis\n  - Pwn\n- AI\n  - Machine Learning\n  - Large Language Model\n\n## CTF Teams\n\n- ICEDTEA (member)\n- NTUT_Is1ab (member)\n\n## Activities\n\n### Community Involvement\n\n- HITCON 2023 (Partner)\n- SITCON 2024 (Attendee)\n- SITCON 2025 (Attendee)\n- HITCON 2025 (Attendee)\n\n### Courses & Training\n\n- AIS3 2023\n- AIS3 Junior 2024\n- AIS3 2025\n- 第九、十屆 AIS3 好厲駭導師培訓學員\n  - 第九屆 AIS3 好厲駭 表現優異獎\n\n## Contact\n\nWelcome to communicate with me, but please tell me who you are first\n\n- Discord: yiwen1825\n- Email: <littlekid0508@gmail.com>\n"},{"title":"categories","url":"/categories/index.html"},{"title":"tags","url":"/tags/index.html"}]