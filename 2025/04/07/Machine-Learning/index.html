<!DOCTYPE html>
<html lang="en">
<style>
    p{
        margin-top: 0 !important;
        margin-bottom: 1rem !important;
        font-size: 1.1rem;
    }
    figure{
        margin: 0 !important;
    }
    pre{
        padding: 0 !important;
        margin-top: 0 !important;
        margin-bottom: 1rem !important;
    }

    td{
        padding: 0 !important;
        margin-bottom: 1rem !important;
    }
    h1,h2,h3,h4,h5,h6{
        margin-top: 0 !important;
        margin-bottom: 1rem !important;
    }
    @media (min-width: 1024px) {
        #middle-box{
            min-width: 56rem;
        }
    
    }
</style>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>y1w3n&#39;s blog - Machine Learning</title>
    <link rel="stylesheet" href="/css/styles.css">
    <link rel="stylesheet" href="/css/tailwind.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/toc.css">
    
<header class="bg-black text-hacker-white py-4 font-dos font-extrabold">
    <div class="container mx-auto flex items-center justify-between space-x-8">
    
      <h1 class="text-2xl font-bold ml-[100px] md:ml-0 animate-pulse text-hacker-color1 md:mr-[20%]">
        <a href="/" class="hover:text-white transition-colors select-none">
          y1w3n&#39;s blog
        </a>
      </h1>
  
    <!-- 大螢幕 -->
      <nav class="hidden md:block">
        <ul class="flex space-x-6">
          
            <li>
              <a href="/" class="select-none text-lg hover:text-hacker-color1 transition-all hover:underline decoration-wavy duration-300 text-hacker-color3">
                Home
              </a>
            </li>
          
            <li>
              <a href="/archives" class="select-none text-lg hover:text-hacker-color1 transition-all hover:underline decoration-wavy duration-300 text-hacker-color3">
                Archives
              </a>
            </li>
          
            <li>
              <a href="/categories" class="select-none text-lg hover:text-hacker-color1 transition-all hover:underline decoration-wavy duration-300 text-hacker-color3">
                Categories
              </a>
            </li>
          
            <li>
              <a href="/tags" class="select-none text-lg hover:text-hacker-color1 transition-all hover:underline decoration-wavy duration-300 text-hacker-color3">
                Tags
              </a>
            </li>
          
            <li>
              <a href="/Friends" class="select-none text-lg hover:text-hacker-color1 transition-all hover:underline decoration-wavy duration-300 text-hacker-color3">
                Friends
              </a>
            </li>
          
            <li>
              <a href="/about" class="select-none text-lg hover:text-hacker-color1 transition-all hover:underline decoration-wavy duration-300 text-hacker-color3">
                About
              </a>
            </li>
          
        </ul>
      </nav>
  
      <!-- 小螢幕 -->
      <button id="menu-toggle" class="block md:hidden text-white focus:outline-none">
        <svg class="w-6 h-6" fill="none" stroke="currentColor" stroke-width="2" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
          <path stroke-linecap="round" stroke-linejoin="round" d="M4 6h16M4 12h16M4 18h16"></path>
        </svg>
      </button>
    </div>
  
    <!-- 摺疊菜單 -->
    <nav id="mobile-menu" class="hidden bg-black">
      <ul class="space-y-2 py-4 px-6">
        
          <li>
            <a href="/" class="block text-white hover:text-hacker-color1 transition-colors">
              Home
            </a>
          </li>
        
          <li>
            <a href="/archives" class="block text-white hover:text-hacker-color1 transition-colors">
              Archives
            </a>
          </li>
        
          <li>
            <a href="/categories" class="block text-white hover:text-hacker-color1 transition-colors">
              Categories
            </a>
          </li>
        
          <li>
            <a href="/tags" class="block text-white hover:text-hacker-color1 transition-colors">
              Tags
            </a>
          </li>
        
          <li>
            <a href="/Friends" class="block text-white hover:text-hacker-color1 transition-colors">
              Friends
            </a>
          </li>
        
          <li>
            <a href="/about" class="block text-white hover:text-hacker-color1 transition-colors">
              About
            </a>
          </li>
        
      </ul>
    </nav>
  
    <!-- RSS Link -->
    <link rel="alternate" type="application/rss+xml" title=" RSS" href="/rss.xml" />
  </header>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.9/lunr.min.js"></script>
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  <script src="/js/search.js"></script>
  <script>
        document.addEventListener("DOMContentLoaded", () => {
    const menuToggle = document.getElementById("menu-toggle");
    const mobileMenu = document.getElementById("mobile-menu");

    menuToggle.addEventListener("click", () => {
        if (mobileMenu.classList.contains("hidden")) {
        mobileMenu.classList.remove("hidden");
        } else {
        mobileMenu.classList.add("hidden");
        }
    });
    });

  </script>
  


    <link rel="icon" href="/yiwen.png">
<meta name="generator" content="Hexo 7.3.0"></head>
<body class="bg-black text-hacker-color3 container mx-auto" style="overflow-x:hidden">
    <!-- 文章標題 -->
    <h1 class="text-5xl text-hacker-color1 font-bold font-dos my-6 text-center">Machine Learning</h1>

    <!-- 發布時間 -->
    <p class="text-hacker-color3 text-center text-sm mb-4">
        2025-04-07
    </p>

    <!-- 文章内容 -->
    <div id="article-content" class="article-entry prose prose-invert mx-auto max-w-4xl leading-relaxed highlight" style="display: flex;">
        <div id="middle-box">
            <h1 id="機器學習"><a href="#機器學習" class="headerlink" title="機器學習"></a>機器學習</h1><blockquote>
<p><strong>BY y1w3n</strong></p>
</blockquote>
<hr>
<blockquote>
<p><strong>隨手記版本</strong></p>
</blockquote>
<p>定義問題-&gt;蒐集資料-&gt;資料分析-&gt;處理資料集-&gt;訓練模型-&gt;推論和預測</p>
<h2 id="名詞"><a href="#名詞" class="headerlink" title="名詞"></a>名詞</h2><ul>
<li><p><mark>監督式學習</mark ></p>
<ul>
<li>透過特徵跟目標變數(正確答案)來學習</li>
<li>KNN<ul>
<li>分類器<ul>
<li>資料放進去之後找離它最近的K筆資料，結果就會推測在被選中的K筆資料中找多的(分類)或平均(回歸)</li>
</ul>
</li>
</ul>
</li>
<li>決策樹<ul>
<li>一棵可用於決策分類的樹形分類器</li>
</ul>
</li>
<li>回歸 ： 預測有意義的數值<ul>
<li>for監督式學習</li>
<li>有意義的數值(例：鞋子尺寸23.5cm、24cm…)</li>
<li>沒意義的數值(例：性別男視為0、女視為1)</li>
</ul>
</li>
<li>分類<ul>
<li><img src="https://hackmd.io/_uploads/r1G_QeDXke.png" alt="image"></li>
</ul>
</li>
</ul>
</li>
<li><p><mark>非監督式學習</mark ></p>
<ul>
<li><strong>不</strong>提供目標變數(正確答案)</li>
<li>所以就要去分析輸入的資料再去解釋分析出的結果</li>
<li>方法：降維(例如PCA)、分群(例：k-means )<ul>
<li>降維：以最少特徵去分析資料的方法</li>
</ul>
</li>
<li>k-means(平均分群演算法)<ol>
<li>設定分群數目</li>
<li>隨機挑選質心</li>
<li>算距離</li>
<li>分群資料(看哪些資料被分到哪個質心)</li>
<li>重找質心(換到每群中間)</li>
<li>重複3~5 一直到質心不再移動</li>
</ol>
</li>
</ul>
</li>
<li><p>強化學習</p>
</li>
</ul>
<h2 id="深度學習入門"><a href="#深度學習入門" class="headerlink" title="深度學習入門"></a>深度學習入門</h2><p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2&hl=zh-tw#scrollTo=WBPaKCarDk0r">一些colab連結</a></p>
<h3 id="感知器神經網路演算法"><a href="#感知器神經網路演算法" class="headerlink" title="感知器神經網路演算法"></a>感知器神經網路演算法</h3><ul>
<li>依照人類大腦特性提出</li>
<li>感知器<ul>
<li><p>接收刺激(input)</p>
</li>
<li><p>神經元計算權重</p>
</li>
<li><p>if超過閾值θ(界限)-&gt; 輸出1 (0訊號不流&#x2F;1流)</p>
</li>
<li><p>if 傳遞訊號 &#x3D; 神經元被激活<br>p.s.神經元只傳遞訊號</p>
</li>
<li><p>主要運用在二元分類</p>
</li>
<li><p>將訊息傳遞給其他神經元作為輸入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 反及閘(NAND)感知器</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">AND</span>(<span class="hljs-params">x1, x2</span>):<br>    x = np.array([x1, x2]) <span class="hljs-comment"># 輸入</span><br>    w = np.array([<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>])<span class="hljs-comment">#權重</span><br>    theta = -<span class="hljs-number">0.7</span><br>    tmp = np.<span class="hljs-built_in">sum</span>(w*x) + theta<br>    <span class="hljs-keyword">if</span> tmp &lt;= <span class="hljs-number">0</span>:<br>       <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>    <span class="hljs-keyword">else</span>:<br>       <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure>
</li>
<li><p>互斥或閘(XOR)在沒有隱藏層運作時不能表示</p>
<ul>
<li>透過組合輸入NAND、OR後再傳遞到AND輸出可達成</li>
<li>就叫<strong>多層感知器</strong></li>
<li>也就是說多層感知器<mark >能表示非線性空間</mark >(非線性空間包含一次以上多項式函數，圖形不是直線)  <details>
    <summary> 舉例</summary>
  x1、x2 : 第0層
  反及閘(NAND)、或閘(OR)輸出 : 第1層
  及閘(AND)輸出 : 第2層

<table>
<thead>
<tr>
<th>x1</th>
<th>x2</th>
<th>NAND</th>
<th>OR</th>
<th>AND</th>
<th>XOR(對照)</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody></table>
  </details></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="神經網路"><a href="#神經網路" class="headerlink" title="神經網路"></a>神經網路</h3><ul>
<li>輸入層-&gt;隱藏層(中間層)-&gt;輸出層</li>
<li><strong>激活函數h(x)</strong> : 將輸入信號轉成輸出信號，必須是非線性函數，這樣疊加層才能發揮優勢(輸出層的激活函數用σ()表示)</li>
<li>神經網路中是<strong>sigmoid函數</strong>作為激活函數</li>
<li><strong>階躍函數</strong> : 激活函數以閾值為界，一旦輸入超過閾值，就切換輸出</li>
</ul>
<details>
  <summary>過程</summary>

<p><img src="https://hackmd.io/_uploads/ByFLmz__0.png" alt="image"></p>
<ul>
<li>a、y : 節點</details></li>
</ul>
<details>
<summary>階躍函數圖形、sigmoid函數圖形</summary>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#階躍函數圖形程式碼</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pylab <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">step_function</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-keyword">return</span> np.array(x &gt; <span class="hljs-number">0</span>)<br><br>x = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br>y = step_function(x)<br>plt.plot(x, y)<br>plt.ylim(-<span class="hljs-number">0.1</span>, <span class="hljs-number">1.1</span>) <span class="hljs-comment"># 指定y軸的範圍</span><br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://hackmd.io/_uploads/S1imzyFuC.png" alt="image"></p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 階躍函數圖形+　sigmoid函數圖形程式碼</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pylab <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">step_function</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.array(x &gt; <span class="hljs-number">0</span>, dtype=<span class="hljs-built_in">int</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br>X = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br>y_step_function = step_function(X)<br>y_sigmoid = sigmoid(X)<br><br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))<br>sns.lineplot(x=X, y=y_step_function, label=<span class="hljs-string">&#x27;step_function&#x27;</span>)<br>sns.lineplot(x=X, y=y_sigmoid, label=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br><br>plt.ylim(-<span class="hljs-number">0.1</span>, <span class="hljs-number">1.1</span>)<br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://hackmd.io/_uploads/SksYdktu0.png" alt="image"></p>
</details>

<ul>
<li>階躍函數跟sigmoid函數比較<ul>
<li>階躍函數只能return 0或1</li>
<li>感知器中神經源之間流動是二元信號(0或1)</li>
<li>神經網路是流動連續數值的信號</li>
<li>輸出信號都在0~1之間</li>
<li>都是非線性函數</li>
</ul>
</li>
<li>ReLU（Rectified Linear Unit）函數<ul>
<li>神經網路最近主要使用的函數</li>
<li>輸出大於0就輸出該值</li>
<li>小於等於0就輸出0</li>
</ul>
</li>
</ul>
<details>
<summary>ReLU函數圖形</summary>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#ReLU函數圖形</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-keyword">return</span> np.maximum(<span class="hljs-number">0</span>, x) <span class="hljs-comment">#maximum: 從輸入中選較大的值輸出</span><br><br>x = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br><span class="hljs-built_in">print</span>(relu(x))<br>y=relu(x)<br>plt.plot(x, y)<br>plt.ylim(-<span class="hljs-number">1</span>, <span class="hljs-number">5</span>) <span class="hljs-comment"># 指定y軸的範圍</span><br>plt.show()<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#print(relu(x))</span><br>[<span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span><br> <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span><br> <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.1</span> <span class="hljs-number">0.2</span> <span class="hljs-number">0.3</span><br> <span class="hljs-number">0.4</span> <span class="hljs-number">0.5</span> <span class="hljs-number">0.6</span> <span class="hljs-number">0.7</span> <span class="hljs-number">0.8</span> <span class="hljs-number">0.9</span> <span class="hljs-number">1.</span>  <span class="hljs-number">1.1</span> <span class="hljs-number">1.2</span> <span class="hljs-number">1.3</span> <span class="hljs-number">1.4</span> <span class="hljs-number">1.5</span> <span class="hljs-number">1.6</span> <span class="hljs-number">1.7</span> <span class="hljs-number">1.8</span> <span class="hljs-number">1.9</span> <span class="hljs-number">2.</span>  <span class="hljs-number">2.1</span><br> <span class="hljs-number">2.2</span> <span class="hljs-number">2.3</span> <span class="hljs-number">2.4</span> <span class="hljs-number">2.5</span> <span class="hljs-number">2.6</span> <span class="hljs-number">2.7</span> <span class="hljs-number">2.8</span> <span class="hljs-number">2.9</span> <span class="hljs-number">3.</span>  <span class="hljs-number">3.1</span> <span class="hljs-number">3.2</span> <span class="hljs-number">3.3</span> <span class="hljs-number">3.4</span> <span class="hljs-number">3.5</span> <span class="hljs-number">3.6</span> <span class="hljs-number">3.7</span> <span class="hljs-number">3.8</span> <span class="hljs-number">3.9</span><br> <span class="hljs-number">4.</span>  <span class="hljs-number">4.1</span> <span class="hljs-number">4.2</span> <span class="hljs-number">4.3</span> <span class="hljs-number">4.4</span> <span class="hljs-number">4.5</span> <span class="hljs-number">4.6</span> <span class="hljs-number">4.7</span> <span class="hljs-number">4.8</span> <span class="hljs-number">4.9</span>]<br></code></pre></td></tr></table></figure>

<p><img src="https://hackmd.io/_uploads/BJEyylK_0.png" alt="image"></p>
</details>

<ul>
<li>運算<ul>
<li>權重的符號<br><img src="https://hackmd.io/_uploads/H1m2XZtdC.png" alt="image"><br><img src="https://hackmd.io/_uploads/B1y3I-Y_A.png" alt="image"><br><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=5NWMewF4gJ1P&line=4&uniqifier=1">colab</a></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">X = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">0.5</span>])<br>W1 = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>]])<br>B1 = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>])<br><span class="hljs-built_in">print</span>(W1.shape) <span class="hljs-comment"># (2, 3)</span><br><span class="hljs-built_in">print</span>(X.shape)  <span class="hljs-comment"># (2,)</span><br><span class="hljs-built_in">print</span>(B1.shape) <span class="hljs-comment"># (3,)</span><br>A1 = np.dot(X, W1) + B1 <span class="hljs-comment"># np.dot : 一次計算出結果</span><br><span class="hljs-built_in">print</span>(A1)<br><span class="hljs-comment">#經過sigmoid激活函數</span><br>Z1=sigmoid(A1)<br><span class="hljs-built_in">print</span>(Z1)<br><br></code></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=WG-XG0WOjDOs&line=1&uniqifier=1">三層神經網路程式</a></p>
<ul>
<li>輸出層<ul>
<li><p>回歸問題用恆等函數(依原樣輸出)</p>
</li>
<li><p>分類問題用softmax函數(但容易有超級大ㄉ值所以用下面的方法修)</p>
<details>
  <summary>這樣修的</summary>

<p><img src="https://hackmd.io/_uploads/ByayAWtu0.png" alt="image"><br>分子和分母上都乘上C這個任意的常數（因為同時對分母和    分子乘以相同的常數，所以計算結果不變）。然後，把這個C    移到指數函數（exp）中，記為logC。最後，把logC替換為    另一個符號C’<br>C通常是取輸入訊號的最大值</p>
</details>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#softmax函數</span><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">a</span>):<br>      c = np.<span class="hljs-built_in">max</span>(a)  <br>      <span class="hljs-comment"># exp(x)是表示ex的指數函數（e是納皮爾常數2.7182...）</span><br>      exp_a = np.exp(a-c)<br>      sum_exp_a = np.<span class="hljs-built_in">sum</span>(exp_a)<br>      y = exp_a / sum_exp_a<br>      <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure>
</li>
<li><p>softmax函數輸出介於0~1且輸出總和為1</p>
</li>
<li><p>softmax函數也可以解釋為<mark>機率</mark></p>
<details>
  <summary>以手寫數字識別為例</summary>

<ul>
<li>分10類-&gt;輸出層神經元數量10個</li>
<li>分類-&gt;softmax函數-&gt;用機率推理結果<br><img src="https://hackmd.io/_uploads/BJ3rxGtOA.png" alt="image"></li>
<li>2的輸出值會最大</details></li>
</ul>
</li>
</ul>
</li>
<li>數據處理<ul>
<li>正規化 : 把資料限定到某個範圍內</li>
<li>預處理 : 對神經網路的輸入數據進行某種既定的轉換</li>
<li>數據白化 : 將資料整體的分佈形狀均勻化的方法</li>
</ul>
</li>
</ul>
<h3 id="神經網路的學習"><a href="#神經網路的學習" class="headerlink" title="神經網路的學習"></a>神經網路的學習</h3><ul>
<li>從<strong>數據</strong>中學習</li>
<li>提取特徵量(人類想的)<ul>
<li>圖像的特徵量通常表示為向量的形式</li>
</ul>
</li>
<li>但深度學習在訓練過程不會有人為介入</li>
<li>數據分為<strong>訓練數據</strong>和<strong>測試數據</strong></li>
<li>泛化能力: 處理未被觀察過的數據的能力</li>
<li>過擬合: 無法處理新數據</li>
<li>one-hot label: 將類別數據轉換為二進制向量表示</li>
</ul>
<h4 id="損失函數"><a href="#損失函數" class="headerlink" title="損失函數"></a>損失函數</h4><p> <a target="_blank" rel="noopener" href="https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8-loss-function-2dcac5ebb6cb">參考連結</a></p>
<ul>
<li>損失就是「實際值和預測值的殘差」</li>
<li>可以用任意函數</li>
<li>越接近0越好</li>
<li>一般用MSE(均方誤差)跟交叉熵誤差</li>
<li><mark>表示性能不好的程度</mark></li>
<li>均方誤差(Mean-Square Error)<ul>
<li><p>最常被用在回歸上的損失函數<br><img src="https://hackmd.io/_uploads/S1NwRGYdC.png" alt="image"></p>
</li>
<li><p>求預測值與真實值之間距離的平方總和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_squared_error</span>(<span class="hljs-params">y, t</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * np.<span class="hljs-built_in">sum</span>((y-t)**<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=TGBChgU75EmJ&line=11&uniqifier=1">均方誤差舉例colab</a></p>
</li>
</ul>
</li>
<li>交叉熵誤差<br>  <img src="https://hackmd.io/_uploads/rkurZQY_C.png" alt="image"><ul>
<li><p>分類問題常用的損失函數</p>
</li>
<li><p>正解標籤越大，交叉熵誤差算出來越接近0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy_error</span>(<span class="hljs-params">y, t</span>): <br>  delta = <span class="hljs-number">1e-7</span> <span class="hljs-comment">#防止負無限大發生</span><br>  <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(t * np.log(y + delta))<br></code></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=F3G1dUL879Dk&line=12&uniqifier=1">交叉熵誤差舉例colab</a></p>
</li>
</ul>
</li>
<li>mini-batch學習 : 從一堆數據中選一批據進行學習</li>
</ul>
<h4 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h4><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Dhou27Ergkk">偏導數跟梯度動畫圖</a></p>
<ul>
<li>目的: 尋找梯度為0的地方，找到的就是最小值或極小值(鞍點)</li>
<li>梯度: 損失函數衡量模型預測與真實值之間的誤差，而梯度是用來最小化損失函數的工具。也就是說要<mark>找最優參數組合</mark>(權重跟偏置)</li>
<li>導數: 某個函數在某一特定點的瞬間變化率<ul>
<li>導數計算方式:對於一般的多項式 ( ax^n )，其導數是 ( anx^(n-1) )。</li>
</ul>
</li>
</ul>
<details>
<summary>簡單解釋一下梯度</summary>
梯度可以用比較簡單的方式理解成「多變數函數的斜率」。假設你爬山的時候有一張地圖，上面標出了高度變化的等高線。梯度就像是告訴你在某個位置，哪個方向是上坡最陡、上升最快的方向。

<p>想像一下你在一個山坡上，你想知道哪個方向爬得最快，梯度就告訴你哪個方向坡最陡，並且告訴你這個方向的陡峭程度。</p>
<p>在數學上，假設有一個二維的高度函數 f(x,y)，梯度是由函數在x和 y 方向上的斜率（即偏導數）組成的向量∇𝑓(𝑥,𝑦)，它由兩部分組成：x 方向上的變化率（偏導數）和在y方向上的變化率（偏導數）。公式是：<br><img src="https://hackmd.io/_uploads/ByKbJnJK0.png" alt="image"><br><strong>∂f&#x2F;∂x: 在y固定的情況下f的變化量除以x的變化量就叫做f對於x的偏導數</strong> (這個符號∂念:partial)<br><img src="https://hackmd.io/_uploads/Syzwl59t0.png" alt="image"></p>
<p>使用偏導數在計算梯度的時候就像是在算每個點的斜率，我們要的就是用梯度找這個範圍的最小值</p>
<p>導數跟偏導數差別:<br>導數單變量函數、單一方向變化<br>偏導數看多個方向上的變化</p>
</details>

<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=4r8QQ_eZkuCE&line=24&uniqifier=1">梯度下降法colab</a></p>
<ul>
<li>算出梯度後，在梯度下降法中，帶入公式:<br><img src="https://hackmd.io/_uploads/SymBBpJtR.png" alt="image"><ul>
<li>θ是參數向量。</li>
<li>α 是學習率。</li>
<li>( ∇θ J(θ))是損失函數(J(θ))對參數θ的梯度。</li>
</ul>
</li>
<li>學習率<ul>
<li>一開始我們要人工設學習率，通常是0.01或0.01</li>
<li>概念: 就好比要找的一個「好位置」，如果學習率太大的話容易跳過那個「好位置」，太小的話雖然準確但會非常慢甚至陷在局部最小值</li>
<li>什麼情況要更新學習率?<ul>
<li>調小<ul>
<li>損失在某段時間內沒有減少時</li>
<li>損失上下波動很大也就是不穩定時</li>
</ul>
</li>
<li>調大<ul>
<li>當損失函數下降得太慢時</li>
<li>陷入局部最小值時</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>帶入上面的公式我會得到新的參數，然後再去計算損失函數跟梯度，就這樣重複</li>
</ul>
<h4 id="神經網路學習完整步驟"><a href="#神經網路學習完整步驟" class="headerlink" title="神經網路學習完整步驟"></a>神經網路學習完整步驟</h4><ul>
<li>目的: 找到合適的參數，也就是合適的偏置跟權重</li>
</ul>
<hr>
<ol>
<li>mini-batch<ul>
<li>從一堆數據中隨機選一批據進行學習，目標是縮小mini-batch損失函數的值</li>
</ul>
</li>
<li>計算梯度<ul>
<li>為了減少mini-batch的損失函數的值，需要求出各個權重參數的梯度。</li>
<li>梯度顯示了在當前 mini-batch 中，損失函數減少最快的方向。</li>
</ul>
</li>
<li>更新參數<ul>
<li>算出梯度後帶入公式更新參數(參數更新是對我當前mini-batch的數據)</li>
</ul>
</li>
<li>重複以上步驟</li>
</ol>
<hr>
<ul>
<li>這裡是用mini-batch做梯度下降所以也叫隨機梯度下降法(Stochastic Gradient Descent, SGD)</li>
<li>簡單但費時<br><img src="https://hackmd.io/_uploads/r1R4TmzFC.png" alt="image"></li>
<li>每經過一個epoch(當所有數據都被使用過時)，就記錄當下數據跟精確度(損失函數)</li>
</ul>
<hr>
<h3 id="誤差反向傳播法"><a href="#誤差反向傳播法" class="headerlink" title="誤差反向傳播法"></a>誤差反向傳播法</h3><ul>
<li>被算在上面的第2步: 記算梯度</li>
</ul>
<h4 id="計算圖"><a href="#計算圖" class="headerlink" title="計算圖"></a>計算圖</h4><ul>
<li>表示神經網路計算過程的結構化圖形<br><img src="https://hackmd.io/_uploads/rycIKSbYA.png" alt="image"></li>
<li>正向傳播: 左到右計算(輸入-&gt; 輸出)</li>
<li>反向傳播: 從右到左，基於鍊式法則後向前計算導數</li>
<li>局部計算: 只需要計算跟自己有關的內容不用考慮全局</li>
<li>計算圖的反向傳播：沿著與正方向相反的方向，乘上局部導數</li>
</ul>
<h4 id="鍊式法則"><a href="#鍊式法則" class="headerlink" title="鍊式法則"></a>鍊式法則</h4><ul>
<li>複合函數: 多個函數組成<br><img src="https://hackmd.io/_uploads/HkR9hr-FA.png" alt="image"></li>
<li>複合函數導數&#x3D;構成這個複合函數的函數們的<strong>導數乘積</strong>(在上面的圖中就是左邊兩個函數的導數相乘&#x3D;右邊函數的導數)</li>
</ul>
<h3 id="相關技巧"><a href="#相關技巧" class="headerlink" title="相關技巧"></a>相關技巧</h3><h4 id="參數更新"><a href="#參數更新" class="headerlink" title="參數更新"></a>參數更新</h4><ul>
<li>隨機梯度下降法(SGD)在某些時候效率低，我們可以優化它的下降路徑<br><img src="https://hackmd.io/_uploads/r1GmLtjY0.png" alt="image"></li>
<li>Momentum<ul>
<li>動量優化法</li>
<li>在0~1之間(通常設0.9)</li>
</ul>
</li>
<li>AdaGrad</li>
<li>Adam</li>
</ul>
<details>

<summary>圖</summary>

<p><img src="https://hackmd.io/_uploads/ryH76KsKC.png" alt="image"><br><img src="https://hackmd.io/_uploads/HkNBpFoKA.png" alt="image"><br><img src="https://hackmd.io/_uploads/BJv8pFsFR.png" alt="image"><br><img src="https://hackmd.io/_uploads/ryfv6KoY0.png" alt="image"></p>
</details>

        </div>
        
            <div class="post-container">
            <aside class="toc-container">
                <button class="toc-toggle">文章目錄</button>
                <nav class="toc" id="toc"></nav>
            </aside>
        </div>
        
    </div>


    <!-- 返回主頁連結 -->
    <div class="text-center my-8">
        <a href="/" class="text-hacker-color1 underline hover:text-hacker-color2">← Back to Home</a>
    </div>

      
      <script src="/js/toc.js"></script>  <!-- 引入 TOC 邏輯 -->
      
      

    <footer class="bg-black text-gray-400 py-4">
    <div class="container mx-auto text-center">
      <p>© <span id="current-year"></span>  Yiwen 
        <br> Powered by <a class="hover:text-white duration-150 hover:underline decoration-slice" target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a class="hover:text-white duration-150 hover:underline decoration-slice" target="_blank" rel="noopener" href="https://github.com/m310ct/hexo-theme-hexploit">Hexpolit</a></p>
        <br> 這個 blog 運行了 <span id="running-time"></span>
        <br> 訪客數 <span id="busuanzi_value_site_uv"></span> | 瀏覽數 <span id="busuanzi_value_site_pv"></span>
    </div>
  </footer>
  
  <script> 
    document.getElementById("current-year").textContent = new Date().getFullYear();
  </script>
  
  <script src="/js/timer.js"></script>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


</body>
</html>
