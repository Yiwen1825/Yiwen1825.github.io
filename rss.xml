<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>y1w3n&#39;s blog</title>
  
  <subtitle>y1w3n&#39;s blog</subtitle>
  <link href="http://yiwen1825.github.io/rss.xml" rel="self"/>
  
  <link href="http://yiwen1825.github.io/"/>
  <updated>2025-04-07T00:14:16.471Z</updated>
  <id>http://yiwen1825.github.io/</id>
  
  <author>
    <name>Yiwen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Machine Learning</title>
    <link href="http://yiwen1825.github.io/2025/04/07/Machine-Learning/"/>
    <id>http://yiwen1825.github.io/2025/04/07/Machine-Learning/</id>
    <published>2025-04-06T17:03:10.000Z</published>
    <updated>2025-04-07T00:14:16.471Z</updated>
    
    <content type="html"><![CDATA[<h1 id="機器學習"><a href="#機器學習" class="headerlink" title="機器學習"></a>機器學習</h1><blockquote><p>BY y1w3n <br>隨手記版本</p></blockquote><p>定義問題-&gt;蒐集資料-&gt;資料分析-&gt;處理資料集-&gt;訓練模型-&gt;推論和預測</p><h2 id="名詞"><a href="#名詞" class="headerlink" title="名詞"></a>名詞</h2><ul><li><p><mark>監督式學習</mark ></p><ul><li>透過特徵跟目標變數(正確答案)來學習</li><li>KNN<ul><li>分類器<ul><li>資料放進去之後找離它最近的K筆資料，結果就會推測在被選中的K筆資料中找多的(分類)或平均(回歸)</li></ul></li></ul></li><li>決策樹<ul><li>一棵可用於決策分類的樹形分類器</li></ul></li><li>回歸 ： 預測有意義的數值<ul><li>for監督式學習</li><li>有意義的數值(例：鞋子尺寸23.5cm、24cm…)</li><li>沒意義的數值(例：性別男視為0、女視為1)</li></ul></li><li>分類<ul><li><img src="https://hackmd.io/_uploads/r1G_QeDXke.png" alt="image"></li></ul></li></ul></li><li><p><mark>非監督式學習</mark ></p><ul><li><strong>不</strong>提供目標變數(正確答案)</li><li>所以就要去分析輸入的資料再去解釋分析出的結果</li><li>方法：降維(例如PCA)、分群(例：k-means )<ul><li>降維：以最少特徵去分析資料的方法</li></ul></li><li>k-means(平均分群演算法)<ol><li>設定分群數目</li><li>隨機挑選質心</li><li>算距離</li><li>分群資料(看哪些資料被分到哪個質心)</li><li>重找質心(換到每群中間)</li><li>重複3~5 一直到質心不再移動</li></ol></li></ul></li><li><p>強化學習</p></li></ul><h2 id="深度學習入門"><a href="#深度學習入門" class="headerlink" title="深度學習入門"></a>深度學習入門</h2><p><a href="https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2&hl=zh-tw#scrollTo=WBPaKCarDk0r">一些colab連結</a></p><h3 id="感知器神經網路演算法"><a href="#感知器神經網路演算法" class="headerlink" title="感知器神經網路演算法"></a>感知器神經網路演算法</h3><ul><li>依照人類大腦特性提出</li><li>感知器<ul><li><p>接收刺激(input)</p></li><li><p>神經元計算權重</p></li><li><p>if超過閾值θ(界限)-&gt; 輸出1 (0訊號不流&#x2F;1流)</p></li><li><p>if 傳遞訊號 &#x3D; 神經元被激活<br>p.s.神經元只傳遞訊號</p></li><li><p>主要運用在二元分類</p></li><li><p>將訊息傳遞給其他神經元作為輸入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 反及閘(NAND)感知器</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">AND</span>(<span class="hljs-params">x1, x2</span>):<br>    x = np.array([x1, x2]) <span class="hljs-comment"># 輸入</span><br>    w = np.array([<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>])<span class="hljs-comment">#權重</span><br>    theta = -<span class="hljs-number">0.7</span><br>    tmp = np.<span class="hljs-built_in">sum</span>(w*x) + theta<br>    <span class="hljs-keyword">if</span> tmp &lt;= <span class="hljs-number">0</span>:<br>       <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>    <span class="hljs-keyword">else</span>:<br>       <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure></li><li><p>互斥或閘(XOR)在沒有隱藏層運作時不能表示</p><ul><li>透過組合輸入NAND、OR後再傳遞到AND輸出可達成</li><li>就叫<strong>多層感知器</strong></li><li>也就是說多層感知器<mark >能表示非線性空間</mark >(非線性空間包含一次以上多項式函數，圖形不是直線)  <details>    <summary> 舉例</summary>  x1、x2 : 第0層  反及閘(NAND)、或閘(OR)輸出 : 第1層  及閘(AND)輸出 : 第2層<table><thead><tr><th>x1</th><th>x2</th><th>NAND</th><th>OR</th><th>AND</th><th>XOR(對照)</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td></tr><tr><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td></tr></tbody></table>  </details></li></ul></li></ul></li></ul><h3 id="神經網路"><a href="#神經網路" class="headerlink" title="神經網路"></a>神經網路</h3><ul><li>輸入層-&gt;隱藏層(中間層)-&gt;輸出層</li><li><strong>激活函數h(x)</strong> : 將輸入信號轉成輸出信號，必須是非線性函數，這樣疊加層才能發揮優勢(輸出層的激活函數用σ()表示)</li><li>神經網路中是<strong>sigmoid函數</strong>作為激活函數</li><li><strong>階躍函數</strong> : 激活函數以閾值為界，一旦輸入超過閾值，就切換輸出</li></ul><details>  <summary>過程</summary><p><img src="https://hackmd.io/_uploads/ByFLmz__0.png" alt="image"></p><ul><li>a、y : 節點</details></li></ul><details><summary>階躍函數圖形、sigmoid函數圖形</summary><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#階躍函數圖形程式碼</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pylab <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">step_function</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-keyword">return</span> np.array(x &gt; <span class="hljs-number">0</span>)<br><br>x = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br>y = step_function(x)<br>plt.plot(x, y)<br>plt.ylim(-<span class="hljs-number">0.1</span>, <span class="hljs-number">1.1</span>) <span class="hljs-comment"># 指定y軸的範圍</span><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://hackmd.io/_uploads/S1imzyFuC.png" alt="image"></p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 階躍函數圖形+　sigmoid函數圖形程式碼</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pylab <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">step_function</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.array(x &gt; <span class="hljs-number">0</span>, dtype=<span class="hljs-built_in">int</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br>X = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br>y_step_function = step_function(X)<br>y_sigmoid = sigmoid(X)<br><br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))<br>sns.lineplot(x=X, y=y_step_function, label=<span class="hljs-string">&#x27;step_function&#x27;</span>)<br>sns.lineplot(x=X, y=y_sigmoid, label=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br><br>plt.ylim(-<span class="hljs-number">0.1</span>, <span class="hljs-number">1.1</span>)<br>plt.legend()<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://hackmd.io/_uploads/SksYdktu0.png" alt="image"></p></details><ul><li>階躍函數跟sigmoid函數比較<ul><li>階躍函數只能return 0或1</li><li>感知器中神經源之間流動是二元信號(0或1)</li><li>神經網路是流動連續數值的信號</li><li>輸出信號都在0~1之間</li><li>都是非線性函數</li></ul></li><li>ReLU（Rectified Linear Unit）函數<ul><li>神經網路最近主要使用的函數</li><li>輸出大於0就輸出該值</li><li>小於等於0就輸出0</li></ul></li></ul><details><summary>ReLU函數圖形</summary><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#ReLU函數圖形</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">x</span>):<br>  <span class="hljs-keyword">return</span> np.maximum(<span class="hljs-number">0</span>, x) <span class="hljs-comment">#maximum: 從輸入中選較大的值輸出</span><br><br>x = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br><span class="hljs-built_in">print</span>(relu(x))<br>y=relu(x)<br>plt.plot(x, y)<br>plt.ylim(-<span class="hljs-number">1</span>, <span class="hljs-number">5</span>) <span class="hljs-comment"># 指定y軸的範圍</span><br>plt.show()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#print(relu(x))</span><br>[<span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span><br> <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span><br> <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.</span>  <span class="hljs-number">0.1</span> <span class="hljs-number">0.2</span> <span class="hljs-number">0.3</span><br> <span class="hljs-number">0.4</span> <span class="hljs-number">0.5</span> <span class="hljs-number">0.6</span> <span class="hljs-number">0.7</span> <span class="hljs-number">0.8</span> <span class="hljs-number">0.9</span> <span class="hljs-number">1.</span>  <span class="hljs-number">1.1</span> <span class="hljs-number">1.2</span> <span class="hljs-number">1.3</span> <span class="hljs-number">1.4</span> <span class="hljs-number">1.5</span> <span class="hljs-number">1.6</span> <span class="hljs-number">1.7</span> <span class="hljs-number">1.8</span> <span class="hljs-number">1.9</span> <span class="hljs-number">2.</span>  <span class="hljs-number">2.1</span><br> <span class="hljs-number">2.2</span> <span class="hljs-number">2.3</span> <span class="hljs-number">2.4</span> <span class="hljs-number">2.5</span> <span class="hljs-number">2.6</span> <span class="hljs-number">2.7</span> <span class="hljs-number">2.8</span> <span class="hljs-number">2.9</span> <span class="hljs-number">3.</span>  <span class="hljs-number">3.1</span> <span class="hljs-number">3.2</span> <span class="hljs-number">3.3</span> <span class="hljs-number">3.4</span> <span class="hljs-number">3.5</span> <span class="hljs-number">3.6</span> <span class="hljs-number">3.7</span> <span class="hljs-number">3.8</span> <span class="hljs-number">3.9</span><br> <span class="hljs-number">4.</span>  <span class="hljs-number">4.1</span> <span class="hljs-number">4.2</span> <span class="hljs-number">4.3</span> <span class="hljs-number">4.4</span> <span class="hljs-number">4.5</span> <span class="hljs-number">4.6</span> <span class="hljs-number">4.7</span> <span class="hljs-number">4.8</span> <span class="hljs-number">4.9</span>]<br></code></pre></td></tr></table></figure><p><img src="https://hackmd.io/_uploads/BJEyylK_0.png" alt="image"></p></details><ul><li>運算<ul><li>權重的符號<br><img src="https://hackmd.io/_uploads/H1m2XZtdC.png" alt="image"><br><img src="https://hackmd.io/_uploads/B1y3I-Y_A.png" alt="image"><br><a href="https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=5NWMewF4gJ1P&line=4&uniqifier=1">colab</a></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">X = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">0.5</span>])<br>W1 = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>]])<br>B1 = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>])<br><span class="hljs-built_in">print</span>(W1.shape) <span class="hljs-comment"># (2, 3)</span><br><span class="hljs-built_in">print</span>(X.shape)  <span class="hljs-comment"># (2,)</span><br><span class="hljs-built_in">print</span>(B1.shape) <span class="hljs-comment"># (3,)</span><br>A1 = np.dot(X, W1) + B1 <span class="hljs-comment"># np.dot : 一次計算出結果</span><br><span class="hljs-built_in">print</span>(A1)<br><span class="hljs-comment">#經過sigmoid激活函數</span><br>Z1=sigmoid(A1)<br><span class="hljs-built_in">print</span>(Z1)<br><br></code></pre></td></tr></table></figure><p><a href="https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=WG-XG0WOjDOs&line=1&uniqifier=1">三層神經網路程式</a></p><ul><li>輸出層<ul><li><p>回歸問題用恆等函數(依原樣輸出)</p></li><li><p>分類問題用softmax函數(但容易有超級大ㄉ值所以用下面的方法修)</p><details>  <summary>這樣修的</summary><p><img src="https://hackmd.io/_uploads/ByayAWtu0.png" alt="image"><br>分子和分母上都乘上C這個任意的常數（因為同時對分母和    分子乘以相同的常數，所以計算結果不變）。然後，把這個C    移到指數函數（exp）中，記為logC。最後，把logC替換為    另一個符號C’<br>C通常是取輸入訊號的最大值</p></details><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#softmax函數</span><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">a</span>):<br>      c = np.<span class="hljs-built_in">max</span>(a)  <br>      exp_a = np.exp(a-c) <span class="hljs-comment"># exp(x)是表示ex的指數函數（e是納皮爾常數2.7182...）</span><br>      sum_exp_a = np.<span class="hljs-built_in">sum</span>(exp_a)<br>      y = exp_a / sum_exp_a<br>      <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure></li><li><p>softmax函數輸出介於0~1且輸出總和為1</p></li><li><p>softmax函數也可以解釋為<mark>機率</mark></p><details>  <summary>以手寫數字識別為例</summary><ul><li>分10類-&gt;輸出層神經元數量10個</li><li>分類-&gt;softmax函數-&gt;用機率推理結果<br><img src="https://hackmd.io/_uploads/BJ3rxGtOA.png" alt="image"></li><li>2的輸出值會最大</details></li></ul></li></ul></li><li>數據處理<ul><li>正規化 : 把資料限定到某個範圍內</li><li>預處理 : 對神經網路的輸入數據進行某種既定的轉換</li><li>數據白化 : 將資料整體的分佈形狀均勻化的方法</li></ul></li></ul><h3 id="神經網路的學習"><a href="#神經網路的學習" class="headerlink" title="神經網路的學習"></a>神經網路的學習</h3><ul><li>從<strong>數據</strong>中學習</li><li>提取特徵量(人類想的)<ul><li>圖像的特徵量通常表示為向量的形式</li></ul></li><li>但深度學習在訓練過程不會有人為介入</li><li>數據分為<strong>訓練數據</strong>和<strong>測試數據</strong></li><li>泛化能力: 處理未被觀察過的數據的能力</li><li>過擬合: 無法處理新數據</li><li>one-hot label: 將類別數據轉換為二進制向量表示</li></ul><h4 id="損失函數"><a href="#損失函數" class="headerlink" title="損失函數"></a>損失函數</h4><p> <a href="https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8-loss-function-2dcac5ebb6cb">參考連結</a></p><ul><li>損失就是「實際值和預測值的殘差」</li><li>可以用任意函數</li><li>越接近0越好</li><li>一般用MSE(均方誤差)跟交叉熵誤差</li><li><mark>表示性能不好的程度</mark></li><li>均方誤差(Mean-Square Error)<ul><li><p>最常被用在回歸上的損失函數<br><img src="https://hackmd.io/_uploads/S1NwRGYdC.png" alt="image"></p></li><li><p>求預測值與真實值之間距離的平方總和</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_squared_error</span>(<span class="hljs-params">y, t</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * np.<span class="hljs-built_in">sum</span>((y-t)**<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><a href="https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=TGBChgU75EmJ&line=11&uniqifier=1">均方誤差舉例colab</a></p></li></ul></li><li>交叉熵誤差<br>  <img src="https://hackmd.io/_uploads/rkurZQY_C.png" alt="image"><ul><li><p>分類問題常用的損失函數</p></li><li><p>正解標籤越大，交叉熵誤差算出來越接近0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy_error</span>(<span class="hljs-params">y, t</span>): <br>  delta = <span class="hljs-number">1e-7</span> <span class="hljs-comment">#防止負無限大發生</span><br>  <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(t * np.log(y + delta))<br></code></pre></td></tr></table></figure><p><a href="https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=F3G1dUL879Dk&line=12&uniqifier=1">交叉熵誤差舉例colab</a></p></li></ul></li><li>mini-batch學習 : 從一堆數據中選一批據進行學習</li></ul><h4 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h4><p><a href="https://www.youtube.com/watch?v=Dhou27Ergkk">偏導數跟梯度動畫圖</a></p><ul><li>目的: 尋找梯度為0的地方，找到的就是最小值或極小值(鞍點)</li><li>梯度: 損失函數衡量模型預測與真實值之間的誤差，而梯度是用來最小化損失函數的工具。也就是說要<mark>找最優參數組合</mark>(權重跟偏置)</li><li>導數: 某個函數在某一特定點的瞬間變化率<ul><li>導數計算方式:對於一般的多項式 ( ax^n )，其導數是 ( anx^(n-1) )。</li></ul></li></ul><details><summary>簡單解釋一下梯度</summary>梯度可以用比較簡單的方式理解成「多變數函數的斜率」。假設你爬山的時候有一張地圖，上面標出了高度變化的等高線。梯度就像是告訴你在某個位置，哪個方向是上坡最陡、上升最快的方向。<p>想像一下你在一個山坡上，你想知道哪個方向爬得最快，梯度就告訴你哪個方向坡最陡，並且告訴你這個方向的陡峭程度。</p><p>在數學上，假設有一個二維的高度函數 f(x,y)，梯度是由函數在x和 y 方向上的斜率（即偏導數）組成的向量∇𝑓(𝑥,𝑦)，它由兩部分組成：x 方向上的變化率（偏導數）和在y方向上的變化率（偏導數）。公式是：<br><img src="https://hackmd.io/_uploads/ByKbJnJK0.png" alt="image"><br><strong>∂f&#x2F;∂x: 在y固定的情況下f的變化量除以x的變化量就叫做f對於x的偏導數</strong> (這個符號∂念:partial)<br><img src="https://hackmd.io/_uploads/Syzwl59t0.png" alt="image"></p><p>使用偏導數在計算梯度的時候就像是在算每個點的斜率，我們要的就是用梯度找這個範圍的最小值</p><p>導數跟偏導數差別:<br>導數單變量函數、單一方向變化<br>偏導數看多個方向上的變化</p></details><p><a href="https://colab.research.google.com/drive/1qEwW2WdXAKM8mVjtRiD52dILROlqW-B3?authuser=2#scrollTo=4r8QQ_eZkuCE&line=24&uniqifier=1">梯度下降法colab</a></p><ul><li>算出梯度後，在梯度下降法中，帶入公式:<br><img src="https://hackmd.io/_uploads/SymBBpJtR.png" alt="image"><ul><li>θ是參數向量。</li><li>α 是學習率。</li><li>( ∇θ J(θ))是損失函數(J(θ))對參數θ的梯度。</li></ul></li><li>學習率<ul><li>一開始我們要人工設學習率，通常是0.01或0.01</li><li>概念: 就好比要找的一個「好位置」，如果學習率太大的話容易跳過那個「好位置」，太小的話雖然準確但會非常慢甚至陷在局部最小值</li><li>什麼情況要更新學習率?<ul><li>調小<ul><li>損失在某段時間內沒有減少時</li><li>損失上下波動很大也就是不穩定時</li></ul></li><li>調大<ul><li>當損失函數下降得太慢時</li><li>陷入局部最小值時</li></ul></li></ul></li></ul></li><li>帶入上面的公式我會得到新的參數，然後再去計算損失函數跟梯度，就這樣重複</li></ul><h4 id="神經網路學習完整步驟"><a href="#神經網路學習完整步驟" class="headerlink" title="神經網路學習完整步驟"></a>神經網路學習完整步驟</h4><ul><li>目的: 找到合適的參數，也就是合適的偏置跟權重</li></ul><hr><ol><li>mini-batch<ul><li>從一堆數據中隨機選一批據進行學習，目標是縮小mini-batch損失函數的值</li></ul></li><li>計算梯度<ul><li>為了減少mini-batch的損失函數的值，需要求出各個權重參數的梯度。</li><li>梯度顯示了在當前 mini-batch 中，損失函數減少最快的方向。</li></ul></li><li>更新參數<ul><li>算出梯度後帶入公式更新參數(參數更新是對我當前mini-batch的數據)</li></ul></li><li>重複以上步驟</li></ol><hr><ul><li>這裡是用mini-batch做梯度下降所以也叫隨機梯度下降法(Stochastic Gradient Descent, SGD)</li><li>簡單但費時<br><img src="https://hackmd.io/_uploads/r1R4TmzFC.png" alt="image"></li><li>每經過一個epoch(當所有數據都被使用過時)，就記錄當下數據跟精確度(損失函數)</li></ul><hr><h3 id="誤差反向傳播法"><a href="#誤差反向傳播法" class="headerlink" title="誤差反向傳播法"></a>誤差反向傳播法</h3><ul><li>被算在上面的第2步: 記算梯度</li></ul><h4 id="計算圖"><a href="#計算圖" class="headerlink" title="計算圖"></a>計算圖</h4><ul><li>表示神經網路計算過程的結構化圖形<br><img src="https://hackmd.io/_uploads/rycIKSbYA.png" alt="image"></li><li>正向傳播: 左到右計算(輸入-&gt; 輸出)</li><li>反向傳播: 從右到左，基於鍊式法則後向前計算導數</li><li>局部計算: 只需要計算跟自己有關的內容不用考慮全局</li><li>計算圖的反向傳播：沿著與正方向相反的方向，乘上局部導數</li></ul><h4 id="鍊式法則"><a href="#鍊式法則" class="headerlink" title="鍊式法則"></a>鍊式法則</h4><ul><li>複合函數: 多個函數組成<br><img src="https://hackmd.io/_uploads/HkR9hr-FA.png" alt="image"></li><li>複合函數導數&#x3D;構成這個複合函數的函數們的<strong>導數乘積</strong>(在上面的圖中就是左邊兩個函數的導數相乘&#x3D;右邊函數的導數)</li></ul><h3 id="相關技巧"><a href="#相關技巧" class="headerlink" title="相關技巧"></a>相關技巧</h3><h4 id="參數更新"><a href="#參數更新" class="headerlink" title="參數更新"></a>參數更新</h4><ul><li>隨機梯度下降法(SGD)在某些時候效率低，我們可以優化它的下降路徑<br><img src="https://hackmd.io/_uploads/r1GmLtjY0.png" alt="image"></li><li>Momentum<ul><li>動量優化法</li><li>在0~1之間(通常設0.9)</li></ul></li><li>AdaGrad</li><li>Adam</li></ul><details><summary>圖</summary><p><img src="https://hackmd.io/_uploads/ryH76KsKC.png" alt="image"><br><img src="https://hackmd.io/_uploads/HkNBpFoKA.png" alt="image"><br><img src="https://hackmd.io/_uploads/BJv8pFsFR.png" alt="image"><br><img src="https://hackmd.io/_uploads/ryfv6KoY0.png" alt="image"></p></details>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;機器學習&quot;&gt;&lt;a href=&quot;#機器學習&quot; class=&quot;headerlink&quot; title=&quot;機器學習&quot;&gt;&lt;/a&gt;機器學習&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;BY y1w3n &lt;br&gt;隨手記版本&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;定義問題-&amp;gt;</summary>
      
    
    
    
    
    <category term="note" scheme="http://yiwen1825.github.io/tags/note/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yiwen1825.github.io/2025/04/05/hello-world/"/>
    <id>http://yiwen1825.github.io/2025/04/05/hello-world/</id>
    <published>2025-04-05T14:59:07.222Z</published>
    <updated>2025-04-05T14:59:07.222Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
